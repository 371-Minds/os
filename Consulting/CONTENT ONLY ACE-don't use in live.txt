CONTENT ONLY: ACE-don't use in live code


Adopting the ACE (Automated Collection Engine) Model
An ACE (Automated Collection Engine) represents a self-contained "blueprint" or recipe for how to collect and process information from a specific source (a subreddit, a Twitter account, a blog, etc.). It's more than just a static API endpoint; it's a dynamic definition that tells our system the what, where, and how of data ingestion for any given target.

This is a powerful concept because it transforms our ingestion pipeline from being hard-coded to being data-driven.

Strategic Benefits of the ACE Model:
Scalability: To add a new data source, we don't deploy new code. We simply add a new ACE definition to our database.
Flexibility: We can have different ACEs for the same source (e.g., one for "hot" posts on a subreddit, another for "new" posts). Each ACE can have a unique AI prompt tailored to its content.
Maintainability: The logic for scraping a specific site is stored as clean, editable data, not buried in multiple code files.
### Upgraded Architecture with ACEs
Let's revise the architecture for the Social Pulse Monitor to incorporate this.

Revised Data Flow Diagram:
mermaid
Copy Code
graph TD
    subgraph "Configuration"
        ACE_DB[DB: ace_definitions]
    end

    subgraph "Pipeline"
        A[Convex Scheduler <br> (Cron Job)] -- 1. gets list of active ACEs from --> ACE_DB;
        A -- 2. schedules one task per ACE --> B(Generic ACE Executor <br> `internalAction`);
        B -- 3. uses ACE definition to fetch from --> C[External Sources <br> (Reddit, Blogs, etc.)];
        B -- 4. saves raw data to --> D{DB: socialPosts_raw};
        B -- 5. schedules analysis --> E(Analysis Function <br> `internalMutation`);
    end
    
    subgraph "Analysis & Display"
        E -- 6. reads from --> D;
        E -- 7. uses ACE prompt to call --> F[AI/LLM Gateway];
        F -- 8. returns analysis --> E;
        E -- 9. saves result to --> G{DB: socialPosts_analyzed};
        H[Admin Dashboard UI] -- 10. subscribes to --> G;
    end
### Implementation Steps
Here is the documentation for implementing this superior ACE-driven model.

Step 1: Update the Database Schema (Again)
We need a new table to store our ACE definitions.

Open convex/schema.ts and add the new ace_definitions table.
typescript
Copy Code
// file: convex/schema.ts (Additions)
// ... other tables

ace_definitions: defineTable({
  sourceName: v.string(), // e.g., "Reddit /r/nocode"
  targetUrl: v.string(), // The URL to scrape
  logic: v.object({ // The "recipe" for scraping
    type: v.union(v.literal("api"), v.literal("html")),
    selectors: v.optional(v.array(v.string())), // e.g., for HTML scraping
  }),
  processingPrompt: v.string(), // The specific AI prompt for this source
  status: v.union(v.literal("active"), v.literal("paused")),
}),
Step 2: Modify the Backend Logic
We'll refactor our socialPulse.ts and crons.ts files to use this new data-driven approach.

Update the Cron Job (convex/crons.ts):
The cron job's only responsibility is now to trigger the main ACE orchestrator.
typescript
Copy Code
// file: convex/crons.ts (Updated)
import { cronJobs } from "convex/server";
import { internal } from "./_generated/api";

const crons = cronJobs();

crons.hourly(
  "runActiveAces", // Updated name
  { hourUTC: 0, minuteUTC: 0 },
  internal.socialPulse.runAllActiveAces, // Trigger the new orchestrator function
);

export default crons;
Update the Pipeline Logic (convex/socialPulse.ts):
This file now contains the generic executor that can handle any ACE.
typescript
Copy Code
// file: convex/socialPulse.ts (Updated)
import { internal } from "./_generated/api";
import { internalAction, internalMutation, internalQuery } from "./_generated/server";
// ... other imports

// 1. The new orchestrator function called by the cron job
export const runAllActiveAces = internalAction({
  handler: async (ctx) => {
    const activeAces = await ctx.runQuery(internal.socialPulse.getActiveAces);
    // Schedule a separate, isolated execution for each active ACE
    for (const ace of activeAces) {
      await ctx.scheduler.runAfter(0, internal.socialPulse.executeAceIngestion, { aceId: ace._id });
    }
  },
});

// Helper query to get active ACEs
export const getActiveAces = internalQuery({
    handler: async (ctx) => {
        return await ctx.db.query("ace_definitions").filter(q => q.eq(q.field("status"), "active")).collect();
    }
});

// 2. The Generic ACE Executor
export const executeAceIngestion = internalAction({
  args: { aceId: v.id("ace_definitions") },
  handler: async (ctx, { aceId }) => {
    const ace = await ctx.runQuery(internal.socialPulse.getAceDefinition, { aceId });
    if (!ace) return;

    // --- Fetch data based on ACE logic ---
    // if (ace.logic.type === 'api') { const data = await fetch(ace.targetUrl) ... }
    // For now, we'll use mock data
    const mockPosts = [{ title: `Data from ${ace.sourceName}` }];

    for (const post of mockPosts) {
      // The rest of the pipeline remains the same
      const rawPostId = await ctx.runMutation(internal.socialPulse.saveRawPost, {
        content: post.title,
        source: ace.sourceName,
      });
      await ctx.scheduler.runAfter(0, internal.socialPulse.analyzePost, {
        rawPostId,
        processingPrompt: ace.processingPrompt, // Pass the specific prompt
      });
    }
  },
});

// Helper to get a single ACE definition
export const getAceDefinition = internalQuery({
    args: { aceId: v.id("ace_definitions") },
    handler: async (ctx, { aceId }) => await ctx.db.get(aceId)
});

// 3. Update the `analyzePost` mutation to accept the prompt
export const analyzePost = internalMutation({
  args: {
    rawPostId: v.id("socialPosts_raw"),
    processingPrompt: v.string(), // The prompt comes from the ACE
  },
  handler: async (ctx, { rawPostId, processingPrompt }) => {
    // ... logic to fetch rawPost ...
    // const analysis = await callLlmGateway(rawPost.content, processingPrompt);
    // ... logic to save analyzed data ...
  },
});

// ... saveRawPost and listAnalyzedPosts functions remain the same ...
By adopting this ACE model, our Social Pulse Monitor transforms from a simple pipeline into a truly scalable and intelligent data ingestion platform. This is a significantly more robust and professional architecture.